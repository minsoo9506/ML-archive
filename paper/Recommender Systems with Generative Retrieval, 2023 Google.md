## Introduction
- 일반적인 MF, two-tower 형태의 retrieval 이 아니라
- candidate ID 를 직접 예측하는 generative retrieval models for sequential recommendation 을 제안
- Transformer Index for GEnerative Recommenders (TIGER)
- Semantic ID 를 새롭게 제안
  - 아이템의 semantic representation 이라고 할 수 있고 아이템 콘텐츠 정보에서 뽑은 sequence of tokens
  - 먼저 pre-trained text encode 로 text embedding 을 만든다.
  - 임베딩에 quantization 하여 a set of ordered tokens/codewords 를 생성한다.
  - 이 Semantic ID 로 transformer sequential recommendation model 을 학습한다.
- Semantic ID 의 장점
  - **의미론적 표현**: 아이템 콘텐츠 정보와 텍스트 임베딩에서 파생되어 아이템의 의미론적 특징을 직접적으로 포착한다.
  - **생성 모델과의 호환성**: 전통적인 검색 방법을 넘어 생성 모델(예: 트랜스포머 순차 추천 모델)을 사용하여 아이템 ID를 직접 예측할 수 있게 한다.
  - **양자화를 통한 효율성/구조화**: 임베딩을 정렬된 토큰/코드워드로 양자화함으로써, 원시 연속 임베딩에 비해 더 구조화되고 잠재적으로 효율적인 처리 또는 저장을 가능하게 한다.

## Proposed Framework
### Semantic ID generation using content features
- a tuple of codewords of length m
- similar items (items with similar content features or whose semantic embeddings are close) should have overlapping Semantic IDs
- 생성 과정
  - 1. 텍스트 임베딩 생성: 사전 학습된 텍스트 인코더(pre-trained text encoder)를 사용하여 각 아이템의 콘텐츠 정보(텍스트)를 고차원의 벡터, 즉 텍스트 임베딩으로 변환합니다.
  - 2. 양자화 (Quantization): 생성된 텍스트 임베딩을 양자화하여 순서가 있는 토큰 또는 코드워드(a set of ordered tokens/codewords) 집합으로 만듭니다. 이 과정을 통해 연속적인 임베딩 공간을 이산적인 토큰의 시퀀스로
    변환할 수 있습니다.

### RQ-VAE란? (논문 그림 참고)
- RQ-VAE는 VQ-VAE (Vector-Quantized VAE)를 확장한 모델로, 고차원의 연속적인 데이터(예: 아이템 임베딩)를 이산적인 토큰(discrete tokens)의 시퀀스로 변환(양자화)하는 데 사용되는 오토인코더 기반의 모델입니다.
- 이름에서 알 수 있듯이 Residual-Quantization (잔차 양자화) 이라는 핵심적인 아이디어를 사용합니다.
- RQ-VAE의 동작 방식 (잔차 양자화)
- RQ-VAE는 한 번에 양자화를 끝내는 것이 아니라, 여러 단계에 걸쳐 점진적으로 양자화를 수행합니다. size K 의 m 개의 codebook 사용.
 1. 1단계 양자화: 입력된 아이템 임베딩과 가장 가까운 코드북(codebook)의 임베딩을 찾고 그 임베딩의 index 가 Semantic ID 의 첫번째 codeword 가 됩니다.
 2. 잔차(Residual) 계산: 원본 임베딩과 1단계에서 찾은 코드워드 사이의 차이(오차)를 계산합니다. 이 차이가 바로 '잔차'입니다.
 3. 2단계 양자화: 1단계에서 계산된 '잔차'를 다시 두 번째 코드북을 사용해 양자화합니다. 즉, 잔차와 가장 가까운 codeword 를 또 찾습니다.
 4. 반복: 이 과정을 여러 번 반복하며, 각 단계의 잔차를 계속해서 다음 단계에서 양자화합니다.
- 이렇게 여러 단계의 양자화를 거치면, 각 단계에서 선택된 코드워드들의 조합(시퀀스)으로 원본 임베딩을 훨씬 더 정밀하게 표현할 수 있습니다. 이것이 바로 Semantic ID가 됩니다.

### RQ-VAE 학습과정
- RQ-VAE의 학습 목표는 인코더(Encoder), 디코더(Decoder), 그리고 여러 개의 코드북(Codebooks)을 동시에 최적화하는 것입니다. 이를 위해 전체 손실 함수(Total Loss)는 크게 세 부분으로 구성됩니다.
- L_total = L_reconstruction + L_codebook + L_commitment
- 이제 각 손실 함수의 역할과 어떻게 학습이 진행되는지 알아보겠습니다.

  1. 재구성 손실 (Reconstruction Loss)

   * 목표: 디코더가 최종적으로 복원한 결과물(x̂)이 원본 입력 데이터(x)와 최대한 비슷해지도록 만드는 것입니다.
   * 역할: Autoencoder의 가장 기본적인 손실 함수로, 인코더와 디코더가 데이터의 핵심 특징을 잘 압축하고 다시 복원하는 능력을 학습시킵니다.
   * 계산: 보통 원본과 복원된 결과물의 Mean Squared Error (MSE)를 사용합니다.
      L_reconstruction = ||x - x̂||²

  2. 코드북 손실 (Codebook Loss)

   * 목표: 코드북에 있는 코드워드(e)들이 인코더의 출력(z)과 가까워지도록 코드북 자체를 학습시키는 것입니다.
   * 역할: 인코더가 어떤 벡터를 출력했을 때, 그 벡터와 가장 가깝다고 선택된 코드워드가 "다음부터는 내가 더 가까이 다가갈게"라고 말하며 자신의 위치를 업데이트하는 것과 같습니다.
   * 계산: 인코더의 출력 z와 선택된 코드워드 e 사이의 거리(MSE)로 계산됩니다.
      L_codebook = ||sg(z) - e||²
       * sg는 stop_gradient를 의미하며, 이 손실은 오직 코드북(`e`)만 업데이트하고 인코더(z)로는 그래디언트가 흐르지 않도록 막습니다.

  3. 결속 손실 (Commitment Loss)

   * 목표: 인코더의 출력(z)이 코드북의 코드워드(e)에서 너무 멀리 벗어나지 않도록, 즉 선택된 코드워드에 "충실하도록(commit)" 인코더를 학습시키는 것입니다.
   * 역할: 코드북이 "내가 이쪽으로 올게"라고 하면(코드북 손실), 인코더는 "나도 그쪽으로 갈게"라고 말하며 서로를 향해 다가가도록 합니다. 인코더의 출력이 제멋대로 무한정 커지거나 변하는 것을
     방지하는 규제(regularization) 역할을 합니다.
   * 계산: 코드북 손실과 형태는 같지만, 그래디언트가 흐르는 대상이 다릅니다.
      L_commitment = β * ||z - sg(e)||²
       * 여기서는 sg가 코드워드(e)에 적용되어, 이 손실은 오직 인코더(`z`)만 업데이트하고 코드북(e)으로는 그래디언트가 흐르지 않습니다. β는 이 손실의 중요도를 조절하는 하이퍼파라미터입니다.

  ---

- RQ-VAE의 학습 과정 종합
- RQ-VAE는 잔차 양자화를 여러 단계(stage)에 걸쳐 수행하므로, 코드북 손실과 결속 손실은 각 단계의 코드북마다 개별적으로 계산된 후 모두 더해집니다.

   1. Forward Pass:
       * 입력 데이터 x가 인코더를 통과해 z_1이 됩니다.
       * z_1은 첫 번째 코드북을 통해 양자화되어 e_1이 됩니다.
       * 원본과의 차이(잔차, x - e_1)가 다시 인코더를 통과해 z_2가 됩니다.
       * z_2는 두 번째 코드북을 통해 양자화되어 e_2가 됩니다.
       * 이 과정이 m번 반복되어 e_1, e_2, ..., e_m이 생성됩니다.
       * 이들을 모두 더한 값(e_1 + ... + e_m)이 디코더에 들어가 최종 결과 x̂를 만듭니다.

   2. Loss Calculation:
       * 위에서 설명한 세 가지 손실(재구성, 코드북, 결속)을 모두 계산합니다. 이때 코드북/결속 손실은 m개의 코드북 전부에 대해 계산하여 합산합니다.

   3. Backward Pass (학습):
       * 계산된 전체 손실로부터 역전파가 진행됩니다.
       * stop_gradient 덕분에 각 손실의 그래디언트가 정확한 대상(인코더, 디코더, 각 코드북)에만 흘러가 각각의 파라미터를 효과적으로 업데이트합니다.

- 이렇게 세 가지 손실이 각자의 역할을 수행하며 상호작용을 통해 인코더, 디코더, 그리고 여러 개의 코드북 모두가 함께 최적화됩니다.

### 코드북 붕괴 (Codebook Collapse) 방지
"RQ-VAE from a codebook collapse, where most of the input gets mapped to only a few codebook vectors, we use k-means clustering-based initialization for the codebook. Specifically, we apply the k-means algorithm on the first training batch and use the centroids as initialization"

위 문장은 **코드북 붕괴(Codebook Collapse) 문제를 해결하기 위한 방법**에 대해 설명하고 있습니다.

1.  **코드북 붕괴란?**
    *   VQ-VAE 계열 모델 학습 시 발생하는 문제로, 다양한 입력 데이터가 들어와도 코드북 내의 **수많은 코드워드들 중 단 몇 개만을 집중적으로 사용하는 현상**을 의미합니다.
    *   **문제점**: 코드북의 대부분이 낭비되어, 모델이 데이터의 다양성을 제대로 표현하지 못하게 됩니다.

2.  **해결책: K-Means 클러스터링 기반 초기화**
    *   이 문제를 해결하기 위해, 코드북의 코드워드들을 **무작위 값으로 시작하는 대신, K-Means 클러스터링 결과를 이용해 "의미 있는" 값으로 초기화**합니다.

3.  **구체적인 실행 방법**
    1.  **첫 학습 데이터 배치 사용**: 학습에 사용할 첫 번째 데이터 배치를 가져옵니다.
    2.  **K-Means 실행**: 이 데이터를 K-Means 알고리즘을 사용해 K개의 그룹(클러스터)으로 나눕니다. (K = 코드북의 크기)
    3.  **중심점(Centroid)을 초기값으로 사용**: 각 클러스터의 중심점(centroid)들을 코드북의 K개 코드워드들의 **초기값**으로 설정합니다.

- **효과**: 이 방법은 코드북이 학습 시작부터 **실제 데이터가 분포하고 있는 중요한 위치들**에 놓이게 하여, 코드워드들이 특정 몇 개에만 쏠리는 현상을 방지하고 코드북 전체를 효과적으로 활용하도록 유도합니다.

### 왜 잔차를 계속 구하면서 진행하는가?
- 잔차(Residual)를 계속 구하면서 여러 단계로 양자화를 진행하는 이유는 크게 **정밀도(Precision)**와 **효율성(Efficiency)** 두 가지 측면에서 설명할 수 있습니다.

1.  **정밀도: 정보 손실을 최소화하기 위해**
    *   **1단계 양자화의 한계**: 아이템의 원래 임베딩 벡터를 코드북에서 가장 가까운 하나의 코드워드로 대체하면, 필연적으로 정보 손실이 발생합니다. 원래 벡터와 선택된 코드워드 사이에는 미세한 차이(오차)가 존재하는데, 이것이 바로 '잔차'입니다. 이 잔차에는 1단계 양자화에서 미처 표현하지 못한 세부 정보가 담겨 있습니다.
    *   **점진적인 세부 묘사**: 잔차를 버리지 않고, 다음 단계에서 이 잔차마저도 가장 가깝게 표현하는 코드워드를 또 찾습니다. 이 과정을 반복하면, 첫 단계에서는 전체적인 의미(예: "운동화")를 잡고, 다음 단계에서는 더 세부적인 속성(예: "나이키", "흰색", "가죽 재질" 등)을 잡아내는 것처럼 원본 임베딩을 점점 더 정밀하게 묘사할 수 있습니다.

2.  **효율성: 거대한 코드북을 작은 코드북 여러 개로 대체하기 위해**
    *   **만약 한 번에 양자화한다면?**: 높은 정밀도를 위해 코드북의 크기(코드워드의 개수)를 매우 크게 만들어야 합니다. 예를 들어 100만 개의 코드워드를 가진 코드북을 사용한다면, 저장 공간도 많이 차지하고 매번 가장 가까운 코드워드를 찾는 계산 비용도 엄청나게 커집니다.
    *   **RQ-VAE의 방식 (작은 코드북 여러 개)**: 만약 1,000개의 코드워드를 가진 코드북 2개를 사용한다고 가정해봅시다.
        *   1단계에서 1,000개 중 하나를 선택합니다.
        *   2단계에서 1,000개 중 하나를 선택합니다.
        *   이 두 코드워드의 조합으로 표현할 수 있는 가짓수는 1,000 x 1,000 = 100만 가지가 됩니다.
    *   **결론**: 즉, **작은 코드북 여러 개를 단계적으로 사용함으로써, 실제로 거대한 코드북을 사용하는 것과 같은 표현력(expressiveness)을 훨씬 적은 계산 비용과 메모리로 달성**할 수 있습니다.

### handling collisions
- Semantic ID 충돌이 발생하면 해당 아이템들에 서로 다른 tokens 를 추가한다.
- 다른 아이템이 (12, 10, 10) 과 (12, 10, 10) 의 결과면 (12, 10, 10, 0), (12, 10, 10, 1) 이런식

### Generative Retrieval with Semantic IDs
- semantic ID 로 seq2eq 형태로 next item 의 semantic ID 를 예측
- generative 하니까 추천 corpus 에 없는 semantic ID 발생할수있으나 거의 없다고 한다. 발생하는 경우 따로 처리.

## Experiment
### RQ-VAE 구현
입력: 아이템의 의미론적 임베딩 (Semantic Embedding)
* 생성 모델: 사전 학습된 Sentence-T5 모델을 사용합니다.
* 입력 정보: 아이템의 title, price, brand, category 같은 콘텐츠 속성을 하나의 문장으로 구성합니다.
* 임베딩 생성: 이 문장을 Sentence-T5 모델에 입력하여 768차원의 의미론적 임베딩 벡터를 얻습니다.

RQ-VAE 모델 아키텍처
* 인코더 (Encoder):
    * 입력된 768차원 임베딩을 받아 3개의 은닉층(512 -> 256 -> 128 차원, ReLU 활성화 함수 사용)을 거쳐 최종적으로 32차원의 잠재 표현(latent
        representation)으로 압축합니다.
* 잔차 양자화기 (Residual Quantizer):
    * 압축된 32차원 표현을 3단계에 걸쳐 양자화합니다.
    * 각 단계마다 256개의 코드워드(codeword)를 가진 코드북을 사용하며, 각 코드워드는 32차원 벡터입니다.
* 디코더 (Decoder): 양자화된 표현을 다시 원래의 의미론적 임베딩 공간으로 복원합니다.

학습 세부사항
* 옵티마이저: Adagrad (학습률: 0.4)
* 배치 사이즈: 1024
* 학습량: 20,000 에폭 동안 학습하여 코드북 활용률을 80% 이상으로 확보합니다.
* 손실 함수: Commitment Loss의 가중치(β)는 0.25를 사용합니다.

Semantic ID 생성
* 학습된 인코더와 양자화기를 사용하여 각 아이템에 대해 3개의 토큰으로 구성된 튜플(3-tuple) 형태의 Semantic ID를 생성합니다.
* 충돌 처리: 만약 서로 다른 아이템이 (7, 1, 4)처럼 동일한 ID를 공유하게 되면, 충돌을 피하기 위해 4번째 고유 코드를 추가합니다. (예: (7, 1, 4,
    0), (7, 1, 4, 1))
* 최종 결과: 충돌이 없는 경우에도 4번째 코드로 0이 할당되어, 결과적으로 모든 아이템은 길이 4의 고유한 Semantic ID를 갖게 됩니다.

### seq2seq 구현
이 모델은 T5X 프레임워크를 사용하여 구현된 트랜스포머 기반의 인코더-디코더 아키텍처입니다.

- 어휘집 (Vocabulary) 구성
  * 아이템 표현: 아이템의 Semantic Codeword를 나타내는 1024개(256 * 4)의 토큰을 포함합니다.
  * 사용자 ID: Hashing Trick을 통해 매핑된 2000개의 사용자 ID 토큰을 추가하여 어휘집 크기를 제한했습니다. 사용자 ID를 입력에 포함함으로써
    추천의 개인화를 강화했습니다.
- 입력 시퀀스 구성
  - 사용자 ID 토큰 뒤에 해당 사용자의 아이템 상호작용 기록에 해당하는 Semantic ID 토큰 시퀀스를 연결하여 입력 시퀀스를 구성합니다.
- 모델 아키텍처 세부사항
  * 레이어 수: 인코더와 디코더 각각 4개의 레이어로 구성됩니다.
  * 셀프 어텐션 헤드: 각 레이어에 6개의 셀프 어텐션 헤드가 있으며, 헤드 차원은 64입니다.
  * 활성화 함수: 모든 레이어에 ReLU를 사용합니다.
  * MLP 및 입력 차원: MLP(다층 퍼셉트론) 차원은 1024, 입력 차원은 128입니다.
  * 드롭아웃: 0.1을 적용합니다.
  * 파라미터 수: 총 약 1300만 개의 파라미터를 가집니다.
- 학습 세부사항
  * 데이터셋별 학습 스텝:
      * "Beauty" 및 "Sports and Outdoors" 데이터셋: 200,000 스텝
      * "Toys and Games" 데이터셋: 100,000 스텝 (데이터셋 크기가 작기 때문)
  * 배치 사이즈: 256
  * 학습률: 처음 10,000 스텝 동안은 0.01을 사용하며, 이후에는 역제곱근 감쇠(inverse square root decay) 스케줄을 따릅니다.

### Cold-start Recommendation
이 섹션에서는 TIGER 프레임워크가 어떻게 새로운 아이템(cold-start item)을 추천하는지에 대한 실험 내용을 다룹니다.

핵심 아이디어:
기존 모델들은 아이템에 임의의 ID를 부여하기 때문에 학습 데이터에 없던 새로운 아이템을 추천할 수 없습니다. 반면, TIGER 프레임워크는 아이템의 의미론적 정보(semantics)를 활용하여 ID를
생성하므로, 사용자 반응이 없었던 새로운 아이템도 추천이 가능합니다.

실험 방법:
1. 데이터셋: Amazon의 'Beauty' 리뷰 데이터를 사용했습니다.
2. 새로운 아이템 시뮬레이션: 학습 데이터에서 테스트 아이템의 5%를 의도적으로 제거하여 '본 적 없는 아이템(unseen items)'으로 간주했습니다.
3. Semantic ID 생성:
    * 아이템 ID는 4개의 토큰으로 구성됩니다. (앞 3개는 RQ-VAE로 생성, 4번째는 고유성 보장용)
    * 학습 데이터로 RQ-VAE와 시퀀스 모델을 학습시킨 후, 이 RQ-VAE를 이용해 학습 데이터에 없던 새로운 아이템을 포함한 모든 아이템의 'Semantic ID'를 생성합니다.
4. 추천 방식:
    * 모델이 특정 'Semantic ID' (c1, c2, c3, c4)를 예측하면, 먼저 이 ID와 정확히 일치하는 학습된 아이템을 후보로 찾습니다.
    * 그 다음, 앞 3개의 토큰 `(c1, c2, c3)`이 일치하는 '본 적 없는 아이템'도 추가로 후보 리스트에 포함시킵니다.
    * ϵ(엡실론)이라는 하이퍼파라미터를 두어, 최종 추천 목록에서 '본 적 없는 아이템'이 차지하는 최대 비율을 조절합니다.

실험 결과:
* TIGER 프레임워크는 의미론적 정보 기반의 KNN(Semantic_KNN) 방식과 비교되었습니다.
* ϵ=0.1로 설정했을 때, TIGER는 모든 Recall@K 지표에서 KNN 방식보다 뛰어난 성능을 보였습니다.
* ϵ 값을 0.1 이상으로 설정한 모든 경우에도 TIGER가 더 우수한 성능을 나타냈습니다.

### Recommendation diversity
- hyperparameter 조절로 다양성도 증가가능
- semantic Id 의